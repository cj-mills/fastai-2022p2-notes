{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf394006",
   "metadata": {},
   "source": [
    "**Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8676f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install -U torch torchvision torchaudio\n",
    "# !pip install numpy pandas pillow gradio\n",
    "# !pip install -U cjm_pil_utils cjm_kaggle_utils cjm_pytorch_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6dd92e",
   "metadata": {},
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "import gradio as gr\n",
    "\n",
    "# Import pandas module for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Set options for Pandas DataFrame display\n",
    "pd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\n",
    "pd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n",
    "\n",
    "# Import PyTorch dependencies\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Import utility functions\n",
    "from cjm_kaggle_utils.core import save_kaggle_creds, dl_kaggle\n",
    "from cjm_pil_utils.core import resize_img, get_img_files, stack_imgs\n",
    "from cjm_pytorch_utils.core import pil_to_tensor, get_torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267bc40",
   "metadata": {},
   "source": [
    "**Set device and data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_torch_device()\n",
    "dtype = torch.float16 if device == 'cuda' else torch.float32\n",
    "device, dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf73613",
   "metadata": {},
   "source": [
    "**Enter Kaggle username and API token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"\"\n",
    "key = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e180fe",
   "metadata": {},
   "source": [
    "**Save Kaggle credentials to file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74065c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_kaggle_creds(username, key, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240b851",
   "metadata": {},
   "source": [
    "**Define directory paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3122017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to store datasets\n",
    "dataset_dir = Path(\"/mnt/980_1TB_2/Datasets/\")\n",
    "# Create the dataset directory if it does not exist\n",
    "dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Dataset Directory: {dataset_dir}\")\n",
    "\n",
    "# Define path to store archive files\n",
    "archive_dir = dataset_dir/'../Archive'\n",
    "# Create the archive directory if it does not exist\n",
    "archive_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Archive Directory: {archive_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3567e503",
   "metadata": {},
   "source": [
    "**Define Kaggle dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the dataset\n",
    "dataset_name = 'hagrid-classification-512p-no-gesture-150k'\n",
    "# dataset_name = 'hagrid-classification-512p-no-gesture-300k'\n",
    "# dataset_name = 'hagrid-classification-512p-no-gesture'\n",
    "\n",
    "# Construct the Kaggle dataset name by combining the username and dataset name\n",
    "kaggle_dataset = f'innominate817/{dataset_name}'\n",
    "\n",
    "# Create the path to the zip file that contains the dataset\n",
    "archive_path = Path(f'{archive_dir}/{dataset_name}.zip')\n",
    "print(f\"Archive Path: {archive_path}\")\n",
    "\n",
    "# Create the path to the directory where the dataset will be extracted\n",
    "dataset_path = Path(f'{dataset_dir}/{dataset_name}')\n",
    "print(f\"Dataset Path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830268d",
   "metadata": {},
   "source": [
    "**Download Kaggle dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_kaggle(kaggle_dataset, archive_path, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c32472",
   "metadata": {},
   "source": [
    "**Get image classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [folder for folder in dataset_path.glob('*/') if folder.is_dir()]\n",
    "\n",
    "class_names = [f.name for f in folders]\n",
    "\n",
    "# print the list of class names\n",
    "pd.DataFrame(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296b672",
   "metadata": {},
   "source": [
    "**Get image paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70dd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [get_img_files(folder) for folder in folders]\n",
    "img_paths = [path for class_paths in img_paths for path in class_paths]\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab22a4",
   "metadata": {},
   "source": [
    "**Display sample image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e287de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random image ID from the list of image IDs\n",
    "img_path = random.choice(img_paths)\n",
    "\n",
    "print(f\"Class: {img_path.parent.name}\")\n",
    "\n",
    "sample_img = Image.open(img_path)\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2f004",
   "metadata": {},
   "source": [
    "**Set training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c48cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_paths_subset = random.sample(img_paths, 70000)\n",
    "img_paths_subset = random.sample(img_paths, len(img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526ab65",
   "metadata": {},
   "source": [
    "**List available ResNet18 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(timm.list_models('resnet18*', pretrained=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783f49c",
   "metadata": {},
   "source": [
    "**Inspect config for ResNet18 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the convnext module\n",
    "from timm.models import resnet\n",
    "\n",
    "# Choose the resnet model\n",
    "resnet_model = 'resnet18d'\n",
    "\n",
    "# Get the default configuration of the chosen model as a Pandas DataFrame\n",
    "pd.DataFrame.from_dict(resnet.default_cfgs[resnet_model], orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b050700",
   "metadata": {},
   "source": [
    "**Load ResNet18 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resenet model\n",
    "resnet18 = timm.create_model(resnet_model, pretrained=True, num_classes=len(class_names))\n",
    "\n",
    "# Set the device and data type\n",
    "resnet18 = resnet18.to(device=device, dtype=dtype).eval()\n",
    "resnet18.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ac7ef",
   "metadata": {},
   "source": [
    "**List available ConvNeXt Nano models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(timm.list_models('convnext_nano*', pretrained=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fdc5b",
   "metadata": {},
   "source": [
    "**Inspect config for convnext model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163a27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the convnext module\n",
    "from timm.models import convnext\n",
    "\n",
    "# Choose the convnext model\n",
    "convnext_model = 'convnext_nano'\n",
    "\n",
    "# Get the default configuration of the chosen model as a Pandas DataFrame\n",
    "pd.DataFrame.from_dict(convnext.default_cfgs[convnext_model], orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b17a12",
   "metadata": {},
   "source": [
    "**Load convnext_nano model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the convnext model\n",
    "convnext_nano = timm.create_model(convnext_model, pretrained=True, num_classes=len(class_names))\n",
    "\n",
    "# Set the device and data type\n",
    "convnext_nano = convnext_nano.to(device=device, dtype=dtype).eval()\n",
    "convnext_nano.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217f402",
   "metadata": {},
   "source": [
    "**Select model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12548c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18\n",
    "# model = convnext_nano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e6942",
   "metadata": {},
   "source": [
    "**Set normalization stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c152dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_stats = resnet.default_cfgs[resnet_model]['mean'], resnet.default_cfgs[resnet_model]['std']\n",
    "norm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c46873",
   "metadata": {},
   "source": [
    "**Set checkpoint directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the checkpoints if it does not already exist\n",
    "checkpoint_dir = Path(\"./hagrid_checkpoints/\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print the checkpoint path\n",
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615753f",
   "metadata": {},
   "source": [
    "**Set checkpoint path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_dir/\"resnet18d-trivial-aug-item-1.pth\"\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34506769",
   "metadata": {},
   "source": [
    "**Load model checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e8ca4",
   "metadata": {},
   "source": [
    "**Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4824af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random image ID from the list of image IDs\n",
    "img_path = random.choice(img_paths)\n",
    "\n",
    "print(f\"Class: {img_path.parent.name}\")\n",
    "\n",
    "sample_img = Image.open(img_path)\n",
    "inp_img = resize_img(sample_img.copy(), 288)\n",
    "\n",
    "img_tensor = pil_to_tensor(inp_img, *norm_stats).to(device=device, dtype=dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "        pred = model(img_tensor)\n",
    "        \n",
    "pred_class = class_names[torch.argmax(torch.sigmoid(pred))]\n",
    "        \n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "\n",
    "\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae70ba8",
   "metadata": {},
   "source": [
    "**Perform inference on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a2095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wrong_imgs = []\n",
    "\n",
    "for path in tqdm(img_paths_subset):\n",
    "    target_cls = path.parent.name\n",
    "\n",
    "    sample_img = Image.open(path)\n",
    "    sample_img = resize_img(sample_img, 288)\n",
    "\n",
    "    img_tensor = pil_to_tensor(sample_img, *norm_stats).to(device=device, dtype=dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            pred = model(img_tensor)\n",
    "\n",
    "    pred_cls = class_names[torch.argmax(torch.sigmoid(pred))]\n",
    "    \n",
    "#     if pred_cls != target_cls: wrong_imgs.append(path)\n",
    "    if pred_cls != target_cls and (pred_cls == \"no_gesture\" or target_cls == \"no_gesture\"): wrong_imgs.append(path)\n",
    "\n",
    "len(wrong_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89f695",
   "metadata": {},
   "source": [
    "**Inspect the number of wrong predictions per class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_imgs_df = pd.DataFrame(wrong_imgs)\n",
    "wrong_imgs_df['class'] = wrong_imgs_df.apply(lambda row: Path(row[0]).parent.stem, axis=1)\n",
    "\n",
    "class_dist_df = wrong_imgs_df['class'].value_counts().to_frame()#.rename(columns={\"class\":run_name})\n",
    "class_dist_df.rename_axis(\"class\", inplace=True)\n",
    "class_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3817864",
   "metadata": {},
   "source": [
    "**Set image paths for gradio interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227444ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_img_paths = wrong_imgs\n",
    "len(gr_img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90047eb6",
   "metadata": {},
   "source": [
    "**Initialize list of images to delete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf43a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_imgs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ea41d",
   "metadata": {},
   "source": [
    "**Initialize list index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542124aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d4eeb",
   "metadata": {},
   "source": [
    "**Define functions for gradio interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec27d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(img_path):\n",
    "    inp_img = resize_img(Image.open(img_path), 288)\n",
    "\n",
    "    img_tensor = pil_to_tensor(inp_img, *norm_stats).to(device=device, dtype=dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            pred = model(img_tensor)\n",
    "\n",
    "    return class_names[torch.argmax(torch.sigmoid(pred))]\n",
    "\n",
    "# Function to go to the previous image\n",
    "def prev_image():\n",
    "    global index\n",
    "    global marked_imgs\n",
    "    index = index - 1 if index > 0 else len(gr_img_paths)-1\n",
    "    img_path = gr_img_paths[index]\n",
    "    btn_val = \"Unmark\" if img_path in marked_imgs else \"Mark to Delete\"\n",
    "    return gr_img_paths[index], gr_img_paths[index].parent.name, get_pred(img_path), btn_val\n",
    "\n",
    "# Function to go to the next image\n",
    "def next_image():\n",
    "    global index\n",
    "    global marked_imgs\n",
    "    index = index + 1 if index < len(gr_img_paths)-1 else 0\n",
    "    img_path = gr_img_paths[index]\n",
    "    btn_val = \"Unmark\" if img_path in marked_imgs else \"Mark to Delete\"\n",
    "    return gr_img_paths[index], gr_img_paths[index].parent.name, get_pred(img_path), btn_val\n",
    "    \n",
    "def mark_to_delete():\n",
    "    global index\n",
    "    global marked_imgs\n",
    "    img_path = gr_img_paths[index]\n",
    "    if img_path in marked_imgs: \n",
    "        marked_imgs.remove(img_path)\n",
    "        return \"Mark to Delete\"\n",
    "    else:\n",
    "        marked_imgs.append(img_path)\n",
    "        return \"Unmark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f03d8",
   "metadata": {},
   "source": [
    "**Create gradio interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38145001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        prev_button = gr.Button('Previous')\n",
    "        next_button = gr.Button('Next')\n",
    "    with gr.Row():\n",
    "        mark_del_button = gr.Button('Mark to Delete')\n",
    "    with gr.Row():\n",
    "        img_class_text = gr.Text(gr_img_paths[index].parent.name, label=\"Image Class\")\n",
    "        pred_class_text = gr.Text(get_pred(gr_img_paths[index]), label=\"Predicted Class\")\n",
    "\n",
    "    image_output = gr.Image(gr_img_paths[index])\n",
    "\n",
    "    prev_button.click(prev_image, outputs=[image_output, img_class_text, pred_class_text, mark_del_button])\n",
    "    next_button.click(next_image, outputs=[image_output, img_class_text, pred_class_text, mark_del_button])\n",
    "    mark_del_button.click(mark_to_delete, outputs=[mark_del_button])\n",
    "        \n",
    "demo.launch(height=1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3687b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92df0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(marked_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52868c",
   "metadata": {},
   "source": [
    "**Delete marked images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf90488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in marked_imgs:\n",
    "    if path.exists():\n",
    "        path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbfc70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
